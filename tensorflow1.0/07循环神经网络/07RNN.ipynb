{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir=os.path.join(os.path.abspath(os.path.dirname('__file__')),'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename,'r') as f:\n",
    "        if sys.version_info[0]>=3:\n",
    "            return f.read().replace('\\n','<eos>').split()\n",
    "        else:\n",
    "            return f.read().decode('utf-8').replace('\\n','<eos>').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab(filename):\n",
    "    words=_read_words(filename)\n",
    "    counter=collections.Counter(words)\n",
    "    word_pairs=sorted(counter.items(),key=lambda x:(-x[1],x[0]))\n",
    "    words,_=list(zip(*word_pairs))\n",
    "    word_to_id=dict(zip(words,range(len(words))))\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_to_words_ids(filename,word_to_id):\n",
    "    data=_read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path=dataDir):\n",
    "    train_path=os.path.join(dataDir,'ptb.train.txt')\n",
    "    valid_path=os.path.join(dataDir,'ptb.valid.txt')\n",
    "    test_path=os.path.join(dataDir,'ptb.test.txt')\n",
    "\n",
    "    word_to_id=_build_vocab(train_path)\n",
    "    train_data=_file_to_words_ids(train_path,word_to_id)\n",
    "    valid_data=_file_to_words_ids(valid_path,word_to_id)\n",
    "    test_data=_file_to_words_ids(test_path,word_to_id)\n",
    "    vocab_len=len(word_to_id)\n",
    "    return train_data,valid_data,test_data,vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data,test_data,vocab_len=ptb_raw_data(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data,batch_size,num_steps,name=None):\n",
    "    with tf.name_scope(name,'PTBProducer',\n",
    "                      [raw_data,batch_size,num_steps]):\n",
    "        raw_data=tf.convert_to_tensor(raw_data,name='raw_data',dtype=tf.int32)\n",
    "        data_len=tf.size(raw_data)\n",
    "        batch_len=data_len//batch_size\n",
    "        data=tf.reshape(raw_data[0:batch_len*batch_size],[batch_size,batch_len])\n",
    "        epoch_size=(batch_len-1)//num_steps\n",
    "        assertion=tf.assert_positive(epoch_size,\n",
    "                                     message='epoch_size==0,decrease batch_size or num_steps')\n",
    "        \n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size=tf.identity(epoch_size,name='epoch_size')\n",
    "        i=tf.train.range_input_producer(epoch_size,shuffle=False).dequeue() \n",
    "        x=tf.strided_slice(data,[0, i*num_steps],[batch_size,(i+1)*num_steps])\n",
    "        x.set_shape([batch_size,num_steps])\n",
    "        \n",
    "        y=tf.strided_slice(data,[0,i*num_steps+1],[batch_size,(i+1)*num_steps+1])\n",
    "        y.set_shape([batch_size,num_steps])\n",
    "        return x,y       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build the rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags=tf.flags\n",
    "logging=tf.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags.DEFINE_string('save_path',None,'Model output directory')\n",
    "# FLAGS=flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIBInput(object):\n",
    "    def __init__(self,config,data,name=None):\n",
    "        self.batch_size=batch_size=config.batch_size\n",
    "        self.num_steps=num_steps=config.num_steps\n",
    "        self.epoch_size=((len(data)//batch_size)-1)//num_steps\n",
    "        self.input_data,self.targets=ptb_producer(data,batch_size,num_steps,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self,is_training,config,input_):\n",
    "        self.input=input_\n",
    "        batch_size=input_.batch_size\n",
    "        num_steps=input_.num_steps\n",
    "        size=config.hidden_size\n",
    "        vocab_size=config.vocab_size\n",
    "        \n",
    "        def lstm_cell():\n",
    "            if 'reuse' in inspect.getargspec(tf.contrib.rnn.BasicLSTMCell.__init__):\n",
    "                return tf.contrib.rnn.BasicLSTMCell(size,forget_bias=0.0,state_is_tuple=True,reuse=tf.get_variable_scope().reuse())\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(size,forget_bias=0.0,state_is_tuple=True)\n",
    "        \n",
    "        attn_cell=lstm_cell\n",
    "        if is_training and config.keep_prob<1:\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(lstm_cell(),output_keep_prob)\n",
    "        \n",
    "        cell=tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)],state_is_tuple=True)\n",
    "        \n",
    "        self.initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding=tf.get_variable('embedding',[vocab_size,size],dtype=tf.float32)\n",
    "            inputs=tf.nn.embedding_lookup(embedding,input_.input_data)\n",
    "        \n",
    "        if is_training and config.keep_prob<1:\n",
    "            inputs=tf.nn.dropout(inputs,config.keep_prob)\n",
    "        \n",
    "        outputs=[]\n",
    "        \n",
    "        state=self.initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:tf.get_variable_scope().reuse_variables()\n",
    "                ((cell_output,state))=cell(inputs[:,time_step,:],state)\n",
    "                outputs.append(cell_output)\n",
    "                \n",
    "        output=tf.reshape(tf.stack(axis=1,values=outputs),[-1,size])\n",
    "        \n",
    "        softmax_w=tf.get_variable('softmax_w',[size,vocab_size],dtype=tf.float32)\n",
    "        softmax_b=tf.get_variable('softmax_b',[vocab_size],dtype=tf.float32)\n",
    "        logits=tf.matmul(output,softmax_w)+softmax_b\n",
    "        \n",
    "        # reshape logits to be 3-D tensor for sequence loss\n",
    "        logits=tf.reshape(logits,[batch_size,num_steps,vocab_size])\n",
    "        \n",
    "        #use the contrib sequence loss and average over the batches\n",
    "        loss=tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                             input_.targets,\n",
    "                                             tf.ones([batch_size,num_steps],dtype=tf.float32),\n",
    "                                             average_across_timesteps=False,\n",
    "                                             average_across_batch=True)\n",
    "        \n",
    "        # update the cost variables\n",
    "        self.cost=cost=tf.reduce_sum(loss)\n",
    "        self.final_state=state\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        self.lr=tf.Variable(0.0, trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),\n",
    "                                       config.max_grad_norm)\n",
    "        optimizer=tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op=optimizer.apply_gradients(zip(grads,tvars),\n",
    "                                               global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        \n",
    "        self.new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        self.lr_update=tf.assign(self.lr,self.new_lr)\n",
    "    \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(self.lr_update,feed_dict={self.new_lr:lr_value})\n",
    "        \n",
    "    \n",
    "class SmallConfig(object):\n",
    "    init_scale=0.1\n",
    "    learning_rate=1.0\n",
    "    max_grad_norm=5\n",
    "    num_layers=2\n",
    "    num_steps=20\n",
    "    hidden_size=200\n",
    "    max_epoch=4\n",
    "    max_max_epoch=13\n",
    "    keep_prob=1.0\n",
    "    lr_delay=0.5\n",
    "    batch_size=20\n",
    "    vocab_size=10000\n",
    "\n",
    "def run_epoch(session,model,eval_op=None,verbose=False):\n",
    "    start_time=time.time()\n",
    "    costs=0.0\n",
    "    iters=0\n",
    "    state=session.run(model.initial_state)\n",
    "\n",
    "    fetches={'cost':model.cost,\n",
    "             'final_state':model.final_state}\n",
    "\n",
    "    if eval_op is not None:\n",
    "        fetches['eval_op']=eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict={}\n",
    "        for i,(c,h) in enumerate(model.initial_state):\n",
    "            feed_dict[c]=state[i].c\n",
    "            feed_dict[h]=state[i].h\n",
    "\n",
    "        vals=session.run(fetches,feed_dict)\n",
    "        cost=vals['cost']\n",
    "        state=vals['final_state']\n",
    "        \n",
    "        costs+=cost\n",
    "        iters+=model.input.num_steps\n",
    "\n",
    "        if verbose and step%(model.input.epoch_size//10)==0:\n",
    "            print('%0.3f perplexity: %.3f speed: %.0f wps'%(step*1.0/model.input.epoch_size,\n",
    "                                                           np.exp(costs/iters),\n",
    "                                                           (iters*model.input.batch_size/(time.time()-start_time))))\n",
    "\n",
    "\n",
    "    return np.exp(costs/iters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=ptb_raw_data(dataDir)\n",
    "train_data,valid_data,test_data,_=raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=SmallConfig()\n",
    "eval_config=SmallConfig()\n",
    "eval_config.batch_size=1\n",
    "eval_config.num_steps=1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "    with tf.name_scope('Train'):\n",
    "        train_input=PIBInput(config=config,data=train_data,name='TrainInput')\n",
    "        \n",
    "        with tf.variable_scope('Model',reuse=None,initializer=initializer):\n",
    "            m=PTBModel(is_training=True,config=config,input_=train_input)\n",
    "        tf.summary.scalar('Training_Loss',m.cost)\n",
    "        tf.summary.scalar('Learning_Rate',m.lr)\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('Valid'):\n",
    "        valid_input=PIBInput(config=config,data=valid_data,name='ValidInput')\n",
    "        \n",
    "        with tf.variable_scope('Model',reuse=True,initializer=initializer):\n",
    "            mvalid=PTBModel(is_training=False,config=config,input_=valid_input)\n",
    "        tf.summary.scalar('Validation_Loss',mvalid.cost)\n",
    "        \n",
    "    with tf.name_scope('Test'):\n",
    "        test_input=PIBInput(config=config,data=test_data,name='TestInput')\n",
    "        \n",
    "        with tf.variable_scope('Model',reuse=True,initializer=initializer):\n",
    "            mtest=PTBModel(is_training=False,config=eval_config,input_=test_input)\n",
    "    \n",
    "    sv=tf.train.Supervisor()\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_delay=config.lr_delay**max(i+1-config.max_epoch,0.0)\n",
    "            m.assign_lr(session,config.learning_rate*lr_delay)\n",
    "            \n",
    "            print('Epoch: %d Learning rate:%.3f'%(i+1,session.run(m.lr)))\n",
    "            \n",
    "            train_perplexity=run_epoch(session,m,eval_op=m.train_op,verbose=True)\n",
    "            \n",
    "            print('Epoch: %d Training Perplexity: %0.3f'%(i+1,train_perplexity))\n",
    "            \n",
    "            valid_perplexity=run_epoch(session,mvalid)\n",
    "            print('Epoch: %d Valid Perplexity: %0.3f'%(i+1,valid_perplexity))\n",
    "            \n",
    "            test_perplexity=run_epoch(session,mtest)\n",
    "            print('Epoch: %d Testing Perplexity:%0.3f'%(i+1test_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing---tf.stride_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 66  77  88  99 100]\n",
      "  [ 66  77  88  99 100]\n",
      "  [ 66  77  88  99 100]]]\n",
      "\n",
      "[[[ 66  77  88  99 100]\n",
      "  [ 66  77  88  99 100]]]\n",
      "\n",
      "[[[77 88]\n",
      "  [77 88]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "t = tf.constant([[[11, 22, 33, 44, 55], [11, 22, 33, 44, 55], [11, 22, 33, 44, 55]],\n",
    "                 [[66, 77, 88, 99, 100], [66, 77, 88, 99, 100], [66, 77, 88, 99, 100]],\n",
    "                 [[111, 222, 333, 444, 555], [111, 222, 333, 444, 555], [111, 222, 333, 444, 555]]])\n",
    "\n",
    "z1 = tf.strided_slice(t, [1], [-1], [1])\n",
    "z2 = tf.strided_slice(t, [1, 0], [-1, 2], [1, 1])\n",
    "z3 = tf.strided_slice(t, [1, 0, 1], [-1, 2, 3], [1, 1, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z1))\n",
    "    print()\n",
    "    print(sess.run(z2))\n",
    "    print()\n",
    "    print(sess.run(z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing--tf.train.range_input_producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6\n",
    "NUM_EPOCHS=5\n",
    "\n",
    "def input_producer():\n",
    "    array=np.array(range(30))+1\n",
    "    i=tf.train.range_input_producer(NUM_EPOCHS,num_epochs=1,shuffle=False).dequeue()\n",
    "    inputs=tf.slice(array,[i*batch_size],[batch_size])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=input_producer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(inputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
